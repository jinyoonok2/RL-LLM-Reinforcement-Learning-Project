# RLOO (REINFORCE Leave-One-Out) Algorithm Configuration
# Use with: python 07_train_rloo.py --config configs/models/<model>.yaml --algo_config configs/algorithms/rloo.yaml

# Core RLOO hyperparameters
learning_rate: 1.5e-5
batch_size: 8
gradient_accumulation_steps: 8

# Leave-one-out baseline
num_samples: 4               # Number of samples per prompt for LOO baseline
baseline_type: "loo"         # "loo" for leave-one-out, "mean" for simple mean

# Generation parameters
max_new_tokens: 256
temperature: 0.7
top_p: 0.9
do_sample: true

# Training schedule
total_epochs: 100
save_freq: 10                # Save checkpoint every N epochs
eval_freq: 5                 # Evaluate every N epochs

# Reward normalization
normalize_reward: true
reward_clip: 10.0

# Variance reduction
use_reward_whitening: true   # Whiten rewards for variance reduction
clip_grad_norm: 1.0          # Gradient clipping for stability

# Reference model (optional KL penalty)
use_kl_penalty: false        # Add KL penalty term
kl_coef: 0.01                # KL penalty coefficient (if enabled)
