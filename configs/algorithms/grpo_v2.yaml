# GRPO v2 - Training from base model (no SFT initialization, pure RL)
# Use with: python 06_train_grpo.py --policy_ckpt meta-llama/Llama-3.2-3B --algo_config configs/algorithms/grpo_v2.yaml --output_dir outputs/run_002/06_grpo_v2 --from_scratch

# Core GRPO hyperparameters (IDENTICAL to v1 for fair comparison)
learning_rate: 1.5e-5        # Same as v1
batch_size: 4                # Reduced for memory (matches PPO)
gradient_accumulation_steps: 8  # Reduced: effective batch=32 (faster)

# Group-based optimization
group_size: 4                # Same as v1
num_groups_per_batch: 2      # Same as v1

# Loss computation
use_batch_bonus: true
group_baseline: "mean"

# Generation parameters
max_new_tokens: 256
temperature: 0.7
top_p: 0.9
do_sample: true

# Training schedule
total_grpo_epochs: 30        # 30 epochs to match total budget (10 SFT + 20 RL = 30)
save_freq: 5                 # Save every 5 epochs
eval_freq: 5                 # Evaluate every 5 epochs (less frequent)

# Reward normalization
normalize_reward: true
reward_clip: 10.0

# Stability
clip_grad_norm: 1.0
