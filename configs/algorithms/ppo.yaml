# PPO (Proximal Policy Optimization) Algorithm Configuration
# Use with: python 05_train_ppo.py --policy_ckpt outputs/run_002/04_sft_llama3b/best_model --algo_config configs/algorithms/ppo.yaml --output_dir outputs/run_002/05_ppo_v1

# Core PPO hyperparameters
learning_rate: 1.41e-5
mini_batch_size: 1
batch_size: 2
gradient_accumulation_steps: 16  # Maintain effective batch=32

# PPO algorithm parameters
ppo_epochs: 2                # Reduced: 2 passes per batch (faster)
clip_range: 0.2              # Clipping parameter epsilon
clip_range_vf: null          # Value function clipping (null = no clipping)
vf_coef: 0.1                 # Value function loss coefficient

# KL divergence control
kl_coef: 0.05                # KL penalty coefficient
target_kl: 0.1               # Target KL divergence

# Generation parameters
max_new_tokens: 256
temperature: 0.7
top_p: 0.9
do_sample: true

# Training schedule
total_ppo_epochs: 20         # Reduced for lighter training
save_freq: 5                 # Save checkpoint every N epochs
eval_freq: 5                 # Evaluate every 5 epochs (less frequent)

# Reward normalization
normalize_reward: true
reward_clip: 10.0            # Clip rewards to [-reward_clip, +reward_clip]

# Reference model (for KL divergence)
use_reference_model: true    # Keep a frozen copy of the SFT model
