# PPO v2 - More aggressive settings for already strong SFT baseline
# Use with: python 05_train_ppo.py --policy_ckpt outputs/run_001/04_sft_llama3b/best_model --algo_config configs/algorithms/ppo_v2.yaml

# Core PPO hyperparameters (IDENTICAL to v1 for fair comparison)
learning_rate: 1.41e-5       # Same as v1
mini_batch_size: 1
batch_size: 4
gradient_accumulation_steps: 16

# PPO algorithm parameters (IDENTICAL to v1)
ppo_epochs: 4                # Same as v1
clip_range: 0.2              # Same as v1
clip_range_vf: null
vf_coef: 0.1

# KL divergence control (IDENTICAL to v1)
kl_coef: 0.05                # Same as v1
target_kl: 0.1               # Same as v1

# Generation parameters
max_new_tokens: 256
temperature: 0.7
top_p: 0.9
do_sample: true

# Training schedule (IDENTICAL to v1)
total_ppo_epochs: 10         # Same across all versions for fair comparison
save_freq: 2                 # Save every 2 epochs
eval_freq: 2                 # Evaluate every 2 epochs

# Reward normalization
normalize_reward: true
reward_clip: 10.0

# Reference model
use_reference_model: true
