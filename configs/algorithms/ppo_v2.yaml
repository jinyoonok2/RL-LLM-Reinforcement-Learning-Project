# PPO v2 - Training from base model (no SFT initialization, pure RL)
# Use with: python 05_train_ppo.py --policy_ckpt meta-llama/Llama-3.2-3B --algo_config configs/algorithms/ppo_v2.yaml --output_dir outputs/run_002/05_ppo_v2 --from_scratch

# Core PPO hyperparameters (IDENTICAL to v1 for fair comparison)
learning_rate: 1.41e-5       # Same as v1
mini_batch_size: 1
batch_size: 4
gradient_accumulation_steps: 8  # Reduced: effective batch=32 (faster)

# PPO algorithm parameters (IDENTICAL to v1)
ppo_epochs: 2                # Reduced: 2 passes per batch (faster)
clip_range: 0.2              # Same as v1
clip_range_vf: null
vf_coef: 0.1

# KL divergence control (IDENTICAL to v1)
kl_coef: 0.05                # Same as v1
target_kl: 0.1               # Same as v1

# Generation parameters
max_new_tokens: 256
temperature: 0.7
top_p: 0.9
do_sample: true

# Training schedule
total_ppo_epochs: 30         # 30 epochs to match v1 total budget (10 SFT + 20 RL = 30)
save_freq: 5                 # Save every 5 epochs
eval_freq: 5                 # Evaluate every 5 epochs (less frequent)

# Reward normalization
normalize_reward: true
reward_clip: 10.0

# Reference model
use_reference_model: true
