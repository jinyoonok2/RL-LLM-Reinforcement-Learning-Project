model:
  name: meta-llama/Llama-3.2-1B-Instruct
  type: llama
  architecture: LlamaForCausalLM
  hidden_size: 2048
  num_layers: 16
  vocab_size: 128256
lora:
  use_lora: true
  r: 32          # Balanced rank for good learning + memory efficiency
  alpha: 64      # 2x rank scaling
  dropout: 0.05  # Lower dropout for stronger learning
  target_modules:
  - q_proj      # Query projection
  - v_proj      # Value projection  
  - k_proj      # Key projection
  - o_proj      # Output projection
  - gate_proj   # MLP gate projection
  - up_proj     # MLP up projection
  - down_proj   # MLP down projection
training:
  epochs: 5  # Increased epochs
  batch_size: 2  # Reduced for memory efficiency with LoRA rank 32
  gradient_accumulation_steps: 4  # Increased to maintain effective batch size
  learning_rate: 8.0e-05  # Higher LR works well with LoRA
  warmup_steps: 200  # More warmup steps
  max_length: 1536  # Increased for FinQA document coverage
  fp16: false
  bf16: true
  max_grad_norm: 1.0
validation:
  eval_steps: 1546  # Validate at end of each epoch (6185 samples / 4 grad_accum)
  save_steps: 1546  # Save at end of each epoch
  logging_steps: 100
generation:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
paths:
  data_dir: datasets/finqa_processed
  output_dir: outputs/run_001/03_sft
  reward_spec: outputs/run_001/02_rewards/reward_spec.yaml
metadata:
  description: Lightweight 1B model optimized for fast LoRA training
