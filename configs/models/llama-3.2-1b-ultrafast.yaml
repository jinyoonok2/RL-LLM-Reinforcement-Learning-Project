model:
  name: meta-llama/Llama-3.2-1B-Instruct
  type: llama
  architecture: LlamaForCausalLM
  hidden_size: 2048
  num_layers: 16
  vocab_size: 128256
lora:
  use_lora: true
  r: 8           # Ultra-light rank = ~131M params (11% of model)
  alpha: 16      # Scaling factor (2x rank)
  dropout: 0.05  # Lower dropout for faster convergence
  target_modules:
  - q_proj       # Only query and value for maximum speed
  - v_proj
training:
  epochs: 3      # Fewer epochs but faster iterations
  batch_size: 8  # Larger batches for speed
  gradient_accumulation_steps: 2  # Effective batch size of 16
  learning_rate: 1.0e-04  # Higher LR for faster convergence
  warmup_steps: 50        # Shorter warmup
  max_length: 512         # Shorter sequences for speed
  fp16: false
  bf16: true
  max_grad_norm: 1.0
validation:
  eval_steps: 500   # Less frequent validation
  save_steps: 500   # Less frequent saves  
  logging_steps: 50 # More frequent logging
generation:
  max_new_tokens: 64  # Shorter generation for speed
  temperature: 0.7
  top_p: 0.9
paths:
  data_dir: datasets/finqa_processed
  output_dir: outputs/run_001/03_sft_fast
  reward_spec: outputs/run_001/02_rewards/reward_spec.yaml
metadata:
  description: Ultra-fast LoRA config - maximum speed for rapid iteration