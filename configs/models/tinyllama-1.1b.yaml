model:
  name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  type: llama
  architecture: LlamaForCausalLM
  hidden_size: 2048
  num_layers: 22
  vocab_size: 32000
lora:
  use_lora: true
  r: 16          # Rank 16 = ~180M trainable params
  alpha: 32      # Scaling factor (2x rank)
  dropout: 0.05  # Low dropout for stability
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
training:
  epochs: 4      # More epochs for smaller model
  batch_size: 8  # Can handle larger batches
  gradient_accumulation_steps: 2  # Effective batch size of 16
  learning_rate: 5.0e-05  # Stable LR to prevent NaN
  warmup_steps: 150       # More warmup for stability
  max_length: 1536       # Longer for FinQA documents
  fp16: false
  bf16: true
  max_grad_norm: 1.0
validation:
  eval_steps: 800   # More frequent validation
  save_steps: 800   # More frequent saves
  logging_steps: 50
generation:
  max_new_tokens: 96
  temperature: 0.8
  top_p: 0.95
paths:
  data_dir: datasets/finqa_processed
  output_dir: outputs/run_001/03_sft_tinyllama
  reward_spec: outputs/run_001/02_rewards/reward_spec.yaml
metadata:
  description: TinyLlama 1.1B - fastest Llama architecture for rapid iteration