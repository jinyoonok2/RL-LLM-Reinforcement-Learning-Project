model:
  name: microsoft/DialoGPT-medium
  type: gpt2
  architecture: GPT2LMHeadModel
  hidden_size: 1024
  num_layers: 24
  vocab_size: 50257
lora:
  use_lora: true
  r: 16          # Rank 16 = ~33M trainable params (very fast!)
  alpha: 32      # Scaling factor
  dropout: 0.05
  target_modules:
  - c_attn       # GPT-2 uses different module names
  - c_proj
training:
  epochs: 5      # Can train more epochs quickly
  batch_size: 4  # Smaller batch size for stability
  gradient_accumulation_steps: 2  # Reduced for longer context
  learning_rate: 5.0e-05  # Lower LR to prevent NaN loss
  warmup_steps: 100       # More warmup for stability
  max_length: 1536        # Longer context for FinQA documents
  fp16: false             # Disable FP16 to prevent numerical issues
  bf16: true              # Use BF16 instead for better stability
  max_grad_norm: 1.0
validation:
  eval_steps: 400   # Very frequent validation
  save_steps: 400
  logging_steps: 25
generation:
  max_new_tokens: 64  # Short generations for speed
  temperature: 0.9
  top_p: 0.95
paths:
  data_dir: datasets/finqa_processed
  output_dir: outputs/run_001/03_sft_dialogpt
  reward_spec: outputs/run_001/02_rewards/reward_spec.yaml
metadata:
  description: DialoGPT-medium - ultra-fast GPT-2 based model for rapid experimentation