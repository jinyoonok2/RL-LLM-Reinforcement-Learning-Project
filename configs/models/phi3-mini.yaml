model:
  name: microsoft/Phi-3-mini-4k-instruct
  type: phi3
  architecture: Phi3ForCausalLM
  hidden_size: 3072
  num_layers: 32
  vocab_size: 32064
lora:
  use_lora: true
  r: 16          # Rank 16 for good performance/speed balance
  alpha: 32
  dropout: 0.05
  target_modules:
  - qkv_proj     # Phi-3 uses combined QKV projection
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
training:
  epochs: 4
  batch_size: 4
  gradient_accumulation_steps: 4  # Effective batch size of 16
  learning_rate: 8.0e-05
  warmup_steps: 150
  max_length: 1024
  fp16: false
  bf16: true
  max_grad_norm: 1.0
validation:
  eval_steps: 600
  save_steps: 600
  logging_steps: 50
generation:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
paths:
  data_dir: datasets/finqa_processed
  output_dir: outputs/run_001/03_sft_phi3
  reward_spec: outputs/run_001/02_rewards/reward_spec.yaml
metadata:
  description: Phi-3 Mini 3.8B - Microsoft's efficient model optimized for reasoning