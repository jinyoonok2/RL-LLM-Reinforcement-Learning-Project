model:
  name: microsoft/DialoGPT-medium
  type: gpt2
  architecture: GPT2LMHeadModel
  hidden_size: 1024
  num_layers: 24
  vocab_size: 50257
lora:
  use_lora: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
  - c_attn
  - c_proj
training:
  epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-05
  warmup_steps: 100
  max_length: 512
  fp16: true
validation:
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
generation:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
paths:
  data_dir: datasets/finqa_processed
  output_dir: outputs/run_001/03_sft_microsoft_DialoGPT_medium
  reward_spec: outputs/run_001/02_rewards/reward_spec.yaml
metadata:
  generated_by: inspect_model_architecture.py
  all_available_layers:
  - act
  - attn_dropout
  - c_attn
  - c_fc
  - c_proj
  - drop
  - dropout
  - ln_1
  - ln_2
  - ln_f
  - resid_dropout
  - wpe
  - wte
