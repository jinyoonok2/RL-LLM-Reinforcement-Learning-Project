# PPO (Proximal Policy Optimization) Configuration
_base_: base.yaml

# PPO specific settings
ppo:
  # Core PPO hyperparameters
  learning_rate: 1.41e-5
  mini_batch_size: 1
  batch_size: 8
  gradient_accumulation_steps: 8
  
  # PPO algorithm parameters
  ppo_epochs: 4
  clip_range: 0.2
  clip_range_vf: null
  vf_coef: 0.1
  
  # KL divergence control
  kl_coef: 0.05
  target_kl: 0.1
  
  # Generation parameters
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  
  # Training schedule
  total_ppo_epochs: 100
  save_freq: 10

# Reward model settings
reward:
  normalize_reward: true
  reward_clip: 10.0